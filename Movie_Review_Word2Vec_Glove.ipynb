{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Movie_Review_Word2Vec_Glove.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "ybFwDGQyZWp3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Try ebedding, conv, LSTM, as well as Word2Vec and Glove for movie review prediction"
      ]
    },
    {
      "metadata": {
        "id": "gbD32d5QMMfq",
        "colab_type": "code",
        "outputId": "5e612675-0625-42c9-81d7-0cbc95c4375e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 835
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install -q keras\n",
        "!pip install --upgrade gensim"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gensim\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/a4/d10c0acc8528d838cda5eede0ee9c784caa598dbf40bd0911ff8d067a7eb/gensim-3.6.0-cp36-cp36m-manylinux1_x86_64.whl (23.6MB)\n",
            "\u001b[K    100% |████████████████████████████████| 23.6MB 1.5MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (0.19.1)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.11.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.14.6)\n",
            "Collecting smart-open>=1.2.1 (from gensim)\n",
            "  Downloading https://files.pythonhosted.org/packages/4b/1f/6f27e3682124de63ac97a0a5876da6186de6c19410feab66c1543afab055/smart_open-1.7.1.tar.gz\n",
            "Collecting boto>=2.32 (from smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/23/10/c0b78c27298029e4454a472a1919bde20cb182dab1662cec7f2ca1dcc523/boto-2.49.0-py2.py3-none-any.whl (1.4MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.4MB 15.1MB/s \n",
            "\u001b[?25hCollecting bz2file (from smart-open>=1.2.1->gensim)\n",
            "  Downloading https://files.pythonhosted.org/packages/61/39/122222b5e85cd41c391b68a99ee296584b2a2d1d233e7ee32b4532384f2d/bz2file-0.98.tar.gz\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.18.4)\n",
            "Collecting boto3 (from smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/77/ee/c97cb0954134e93c9ce7f0c5fabbd23b3d580036db024297c895bfb6ef28/boto3-1.9.38-py2.py3-none-any.whl (128kB)\n",
            "\u001b[K    100% |████████████████████████████████| 133kB 26.7MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.22)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.6)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2018.10.15)\n",
            "Collecting jmespath<1.0.0,>=0.7.1 (from boto3->smart-open>=1.2.1->gensim)\n",
            "  Downloading https://files.pythonhosted.org/packages/b7/31/05c8d001f7f87f0f07289a5fc0fc3832e9a57f2dbd4d3b0fee70e0d51365/jmespath-0.9.3-py2.py3-none-any.whl\n",
            "Collecting s3transfer<0.2.0,>=0.1.10 (from boto3->smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/14/2a0004d487464d120c9fb85313a75cd3d71a7506955be458eebfe19a6b1d/s3transfer-0.1.13-py2.py3-none-any.whl (59kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 23.2MB/s \n",
            "\u001b[?25hCollecting botocore<1.13.0,>=1.12.38 (from boto3->smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5b/1c/02fe38b978498d8d466d748fe5513f6d6aa88fc28e21aec0b7dc679f6685/botocore-1.12.38-py2.py3-none-any.whl (4.7MB)\n",
            "\u001b[K    100% |████████████████████████████████| 4.8MB 6.4MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.38->boto3->smart-open>=1.2.1->gensim) (2.5.3)\n",
            "Collecting docutils>=0.10 (from botocore<1.13.0,>=1.12.38->boto3->smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/fa/08e9e6e0e3cbd1d362c3bbee8d01d0aedb2155c4ac112b19ef3cae8eed8d/docutils-0.14-py3-none-any.whl (543kB)\n",
            "\u001b[K    100% |████████████████████████████████| 552kB 24.8MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: smart-open, bz2file\n",
            "  Running setup.py bdist_wheel for smart-open ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/23/00/44/e5b939f7a80c04e32297dbd6d96fa3065af89ecf57e2b5f89f\n",
            "  Running setup.py bdist_wheel for bz2file ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/81/75/d6/e1317bf09bf1af5a30befc2a007869fa6e1f516b8f7c591cb9\n",
            "Successfully built smart-open bz2file\n",
            "Installing collected packages: boto, bz2file, jmespath, docutils, botocore, s3transfer, boto3, smart-open, gensim\n",
            "Successfully installed boto-2.49.0 boto3-1.9.38 botocore-1.12.38 bz2file-0.98 docutils-0.14 gensim-3.6.0 jmespath-0.9.3 s3transfer-0.1.13 smart-open-1.7.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vKHaeK9yMfPK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!wget http://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz\n",
        "!tar -xvzf review_polarity.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GWXB2pVeMm9d",
        "colab_type": "code",
        "outputId": "259cf95a-1881-4530-8729-a4b72ba9cc22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from string import punctuation\n",
        "from os import listdir\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from numpy import array,asarray,zeros \n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout,Flatten,Embedding \n",
        "\n",
        "from keras.layers.convolutional import Conv1D,MaxPooling1D\n",
        " "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ucj82e-9OV97",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Define Vocabulary"
      ]
    },
    {
      "metadata": {
        "id": "PL8ECr9VMpg7",
        "colab_type": "code",
        "outputId": "966e5dea-f1ed-4c54-c7a7-12f98cb61464",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        " \n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "\ttokens = doc.split()\n",
        "\ttable = str.maketrans('', '', punctuation)\n",
        "\ttokens = [w.translate(table) for w in tokens]\n",
        "\ttokens = [word for word in tokens if word.isalpha()]\n",
        "\tstop_words = set(stopwords.words('english'))\n",
        "\ttokens = [w for w in tokens if not w in stop_words]\n",
        "\ttokens = [word for word in tokens if len(word) > 1]\n",
        "\treturn tokens\n",
        " \n",
        "# load doc and add to vocab\n",
        "def add_doc_to_vocab(filename, vocab):\n",
        "\tdoc = load_doc(filename)\n",
        "\ttokens = clean_doc(doc)\n",
        "\tvocab.update(tokens)\n",
        "\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab, is_trian):\n",
        "\tfor filename in listdir(directory):\n",
        "\t\tif is_trian and filename.startswith('cv9'):\n",
        "\t\t\tcontinue\n",
        "\t\tif not is_trian and not filename.startswith('cv9'):\n",
        "\t\t\tcontinue\n",
        "\t\tpath = directory + '/' + filename\n",
        "\t\tadd_doc_to_vocab(path, vocab)\n",
        "\n",
        "# define vocab\n",
        "vocab = Counter()\n",
        "\n",
        "# add all docs to vocab\n",
        "process_docs('txt_sentoken/neg', vocab, True)\n",
        "process_docs('txt_sentoken/pos', vocab, True)\n",
        "\n",
        "print(len(vocab))\n",
        "print(vocab.most_common(50))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "44276\n",
            "[('film', 7983), ('one', 4946), ('movie', 4826), ('like', 3201), ('even', 2262), ('good', 2080), ('time', 2041), ('story', 1907), ('films', 1873), ('would', 1844), ('much', 1824), ('also', 1757), ('characters', 1735), ('get', 1724), ('character', 1703), ('two', 1643), ('first', 1588), ('see', 1557), ('way', 1515), ('well', 1511), ('make', 1418), ('really', 1407), ('little', 1351), ('life', 1334), ('plot', 1288), ('people', 1269), ('could', 1248), ('bad', 1248), ('scene', 1241), ('movies', 1238), ('never', 1201), ('best', 1179), ('new', 1140), ('scenes', 1135), ('man', 1131), ('many', 1130), ('doesnt', 1118), ('know', 1092), ('dont', 1086), ('hes', 1024), ('great', 1014), ('another', 992), ('action', 985), ('love', 977), ('us', 967), ('go', 952), ('director', 948), ('end', 946), ('something', 945), ('still', 936)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FU63yvSOPXTh",
        "colab_type": "code",
        "outputId": "72ee8554-e41b-4883-8465-772413f8a46e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#save vocab to a file\n",
        "min_occurane = 2\n",
        "tokens = [k for k,c in vocab.items() if c >= min_occurane]\n",
        "print(len(tokens))\n",
        "\n",
        "def save_list(lines, filename):\n",
        "\tdata = '\\n'.join(lines)\n",
        "\tfile = open(filename, 'w')\n",
        "\tfile.write(data)\n",
        "\tfile.close()\n",
        "  \n",
        "save_list(tokens, 'vocab_movie.txt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25767\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0FRskXUBNuMP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        " \n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc, vocab):\n",
        "\t# split into tokens by white space\n",
        "\ttokens = doc.split()\n",
        "\t# remove punctuation from each token\n",
        "\ttable = str.maketrans('', '', punctuation)\n",
        "\ttokens = [w.translate(table) for w in tokens]\n",
        "\t# filter out tokens not in vocab\n",
        "\ttokens = [w for w in tokens if w in vocab]\n",
        "\ttokens = ' '.join(tokens)\n",
        "\treturn tokens\n",
        " \n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab, is_trian):\n",
        "\tdocuments = list()\n",
        "\t# walk through all files in the folder\n",
        "\tfor filename in listdir(directory):\n",
        "\t\t# skip any reviews in the test set\n",
        "\t\tif is_trian and filename.startswith('cv9'):\n",
        "\t\t\tcontinue\n",
        "\t\tif not is_trian and not filename.startswith('cv9'):\n",
        "\t\t\tcontinue\n",
        "\t\t# create the full path of the file to open\n",
        "\t\tpath = directory + '/' + filename\n",
        "\t\t# load the doc\n",
        "\t\tdoc = load_doc(path)\n",
        "\t\t# clean doc\n",
        "\t\ttokens = clean_doc(doc, vocab)\n",
        "\t\t# add to list\n",
        "\t\tdocuments.append(tokens)\n",
        "\treturn documents\n",
        " \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w0d_esSCMy7g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# load the vocabulary\n",
        "vocab_filename = 'vocab_movie.txt'\n",
        "vocab = load_doc(vocab_filename)\n",
        "vocab = vocab.split()\n",
        "vocab = set(vocab)\n",
        "\n",
        "# load all training reviews\n",
        "train_positive_docs = process_docs('txt_sentoken/pos', vocab, True)\n",
        "train_negative_docs = process_docs('txt_sentoken/neg', vocab, True)\n",
        "train_docs = train_negative_docs + train_positive_docs\n",
        "\n",
        "# create the tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_docs)\n",
        "test_encoded_docs = tokenizer.texts_to_sequences(train_docs)\n",
        "max_length = max([len(s.split()) for s in train_docs])\n",
        "Xtrain = pad_sequences(test_encoded_docs, maxlen=max_length, padding='post')\n",
        "ytrain = array([0 for _ in range(len(train_negative_docs))] + [1 for _ in range(len(train_positive_docs))])\n",
        "\n",
        "\n",
        "# load all test reviews\n",
        "test_positive_docs = process_docs('txt_sentoken/pos', vocab, False)\n",
        "test_negative_docs = process_docs('txt_sentoken/neg', vocab, False)\n",
        "test_docs = test_negative_docs + test_positive_docs\n",
        "test_encoded_docs = tokenizer.texts_to_sequences(test_docs) #reuse the tokens from training set\n",
        "Xtest = pad_sequences(test_encoded_docs, maxlen=max_length, padding='post')\n",
        "ytest = array([0 for _ in range(len(test_negative_docs))] + [1 for _ in range(len(test_positive_docs))])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eoM33V-9QR3r",
        "colab_type": "code",
        "outputId": "823f1024-931a-4de4-a007-c88b7797553a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        }
      },
      "cell_type": "code",
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# define model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 100, input_length=max_length))\n",
        "model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(10, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(Xtrain, ytrain, epochs=5, verbose=2)\n",
        "# evaluate\n",
        "loss, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
        "print('Test Accuracy: %f' % (acc*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_5 (Embedding)      (None, 1317, 100)         2576800   \n",
            "_________________________________________________________________\n",
            "conv1d_5 (Conv1D)            (None, 1310, 32)          25632     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 1310, 32)          0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_3 (MaxPooling1 (None, 655, 32)           0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 655, 32)           0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 20960)             0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 10)                209610    \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 2,812,053\n",
            "Trainable params: 2,812,053\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            " - 20s - loss: 0.6899 - acc: 0.5328\n",
            "Epoch 2/5\n",
            " - 20s - loss: 0.6454 - acc: 0.6367\n",
            "Epoch 3/5\n",
            " - 20s - loss: 0.4264 - acc: 0.8650\n",
            "Epoch 4/5\n",
            " - 20s - loss: 0.0899 - acc: 0.9778\n",
            "Epoch 5/5\n",
            " - 20s - loss: 0.0091 - acc: 1.0000\n",
            "Test Accuracy: 88.500000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eqjiyhORRcHT",
        "colab_type": "code",
        "outputId": "dd0f5115-4180-4d5f-d6ad-4c39c669c476",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "test_positive_docs[1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'director dominic made highly underrated kalifornia producer jerry bruckheimer rock armageddon bring us slick entertaining remake film name absolutely one ever seen nicolas cage plays memphis retired car thief whos pulled back business evil car thief christopher eccleston determined kill memphis kid brother giovanni ribisi memphis ordered steal cars four days time brother meet unfortunate demise elude detectives hot trail rival car thief feels job given gang memphis sets put old crew back together discovers retired well gone sixty seconds things right opening credits sequence get rockin little tune moby along simple back story told photographs assorted objects filmmakers sometimes make break film opening title sequence one easily gets mindset entertaining ride follows doesnt disappoint cage turns one good performances easily go either way good bad dont know mannerisms dialogue delivery carry film along nicely supporting cast members angelina jolie memphis former love film provide eye candy shes definitely hottest looking grease monkey ive ever seen robert duvall memphis former mentor around lend film class also im big fan patton armageddon postman would love see get huge role someday none three performers given much unfortunately underrated performers however given meaty supporting roles delroy lindo get shorty shines exasperated detective pursuit memphis timothy olyphant go partner wish characters could important real gripe film though conclusion mainly know end opening credits even roll theres doubt anyones mind cars successfully stolen filmmakers blew perfectly good opportunity add suspense picture using rival car thief plot line stands story line wrapped halfway film tidy little package making film id rival gang trying get cars memphis crew thereby making memphis improvise thereby adding meat plot finale essentially big car chase arent ways make car chases interesting anymore done chase also shot edited jerry bruckheimer action sequence kind way leaves audience wondering specifically going scene sure car chase exactly particulars hard tell finally theres stunt scene comes close challenging bus jumping sequence speed oh dont think department despite minor complaints gone sixty seconds pure summer movie entertainment thought provoking shiny loud fun summer flick'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "metadata": {
        "id": "gL0Lb8L5W0l4",
        "colab_type": "code",
        "outputId": "cc1eaeeb-ea49-4f23-df57-410ba2914513",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "model.predict_classes(Xtest)[1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "metadata": {
        "id": "APMNxQPOXIcd",
        "colab_type": "code",
        "outputId": "b1082fae-2f86-4969-9f75-4d278bde9b01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "ytest[1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "metadata": {
        "id": "8HWtrUAZZx6B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Word Embedding\n",
        "\n",
        "A word embedding is an way to represent words using a vctor representation (not like One-hot encoding, but they have same purpose)\n",
        "\n",
        "The values in the embedding vector are learned from text and ware based on the surrounding words. Two popular examples of embeddings are: Word3Vec and Glove. Two popular methods to train the embeddings are bag-of-word and skip-gram. Please refer to the Stanford Natural Language Processing course for details, which is the best course that I found. http://web.stanford.edu/class/cs224n/\n",
        "\n",
        "Keras offers embedding layers (https://keras.io/layers/embeddings/#embedding) that can be easily added to your neural networks. \n",
        "\n",
        "keras.layers.Embedding(input_dim, output_dim, embeddings_initializer='uniform', \n",
        "                       embeddings_regularizer=None, activity_regularizer=None, \n",
        "                       embeddings_constraint=None, mask_zero=False, input_length=None)\n",
        "\n",
        "The input of this embedding layer is required to be integer encoded (liker one-hot encode). This preparation step is usually conducted by Tokenizer, which is also provided in Keras.\n",
        "\n",
        "\n",
        "![](https://qph.fs.quoracdn.net/main-qimg-3e812fd164a08f5e4f195000fecf988f)"
      ]
    },
    {
      "metadata": {
        "id": "gKoHRL9rZ5JW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Word2Vec\n",
        "\n",
        "The basic idea of Word2Vec or Glove is very simple. They uses existing corpus to generate/training the embeddings, because we agree that \"the meaning of a word can be inferred by the company it keeps\" .  This is similar to the transfer learning in computer vision. We saved the weights (embeddings) from pre-trained networks and than add a few new layers to model the current problem.\n",
        "\n",
        "The word2vec algorithms include skip-gram and CBOW models, using either hierarchical softmax or negative sampling"
      ]
    },
    {
      "metadata": {
        "id": "WTkoooJnXlir",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SyXhcSLxcbKf",
        "colab_type": "code",
        "outputId": "170b32aa-5cf2-48d2-94f2-fa1ba52aeb36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "model = Word2Vec(train_docs, size=100, window=5, workers=8, min_count=1)\n",
        "# summarize vocabulary size in model\n",
        "words = list(model.wv.vocab)\n",
        "print('Vocabulary size: %d' % len(words))\n",
        "\n",
        "# save model in ASCII (word2vec) format\n",
        "filename = 'embedding_word2vec.txt'\n",
        "model.wv.save_word2vec_format(filename, binary=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size: 27\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4ptBJyAzcfg8",
        "colab_type": "code",
        "outputId": "460642cc-b483-4309-ef20-778840e84269",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        }
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "embedding_word2vec.txt\treview_polarity.tar.gz\ttxt_sentoken\n",
            "poldata.README.2.0\tsample_data\t\tvocab_movie.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pX2J9yPPcwph",
        "colab_type": "code",
        "outputId": "9e7dac26-f7a8-4506-f10b-3f1ea4488911",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 604
        }
      },
      "cell_type": "code",
      "source": [
        "cat embedding_word2vec.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "27 100\n",
            "  -0.054297768 0.37111235 0.39346772 -0.059841778 0.2975644 -0.087068796 -0.38315597 0.0013162381 -0.119920604 0.08157335 0.15997213 0.26021948 -0.1754535 0.12761211 0.13447402 -0.02233237 -0.061185177 -0.14506665 -0.13332619 -0.08910539 -0.29466495 0.043603275 0.009737978 -0.067909405 0.32747728 -0.082866676 -0.071710125 0.058093958 -0.025532877 -0.107163884 -0.18632796 0.15541753 0.14906631 0.14483017 0.30382589 0.0378381 0.016284432 0.06754286 0.09453005 -0.11837328 -0.16858798 -0.30822954 0.26654914 -0.28213292 -0.018136185 -0.005222439 0.0047721737 0.100159906 0.17439094 -0.51919156 0.12199433 -0.33687064 -0.42350942 -0.28219742 -0.33948904 0.05306033 0.118537605 0.15010369 -0.004340681 -0.30583522 -0.43255037 0.016526617 -0.17005607 -0.36167505 -0.13703804 -0.05937956 -0.23050828 -0.097290546 -0.10880392 0.09241034 0.07001113 0.3864962 -0.018560275 0.33691743 -0.018140696 0.21017286 -0.15074301 -0.41392353 -0.0071451915 0.15077442 0.027798504 0.082764216 0.13520975 0.37295362 0.48549014 -0.22872807 -0.23352428 0.2941278 0.6641176 0.55554175 0.31343794 -0.10351618 -0.13451648 -0.11614029 -0.11698088 0.008861604 0.04362097 0.1726968 0.13338453 -0.016556207\n",
            "e 0.0049357135 -0.3671511 -0.11595681 -0.31078058 -0.16652913 -0.07850985 0.35929608 -0.23980387 0.0017733925 -0.08651741 -0.25083748 -0.21788003 0.059647676 -0.16374236 -0.0472963 -0.11283893 0.045823533 0.19023533 0.1616355 0.14062373 0.060547348 -0.23510626 -0.12519036 -0.015068397 -0.03589098 0.27038562 0.1593672 -0.21550663 -0.44115222 0.0008694476 0.4111029 0.314849 -0.1896338 0.16673505 -0.083494306 0.4410589 0.12764703 0.1751234 0.23016138 -0.14344189 -0.08613004 -0.16040337 -0.17130601 0.029146189 0.16786326 0.3077975 -0.38035145 -0.18572801 0.0043991725 0.4402098 -0.21215276 0.25811744 0.24964383 0.015544724 0.0005560203 0.0072531565 0.2362368 0.13919707 -0.09445693 -0.055681355 -0.107347704 0.13029411 0.018493472 0.21647517 -0.09964643 0.012268515 -0.20098095 -0.17927241 -0.01779143 -0.07744488 0.0038762304 0.32160267 -0.054293245 -0.011506631 0.0044949143 -0.021554664 0.09396095 0.11121989 -0.0041710143 0.07008177 0.022737976 -0.023013249 -0.23098642 0.15140782 -0.014830285 -0.231834 -0.09811593 0.11098626 0.04466496 0.2848225 0.2467163 0.06577241 -0.23203789 0.20274681 0.22772476 -0.27815077 -0.036797795 -0.06258048 0.1859591 -0.055608895\n",
            "i 0.19382992 0.1164637 -0.34498718 -0.4485391 -0.13867155 0.04116816 0.0496599 0.20066683 0.118571356 0.0649059 -0.27718067 -0.31753314 -0.081646435 -0.13577078 0.40170887 0.108442634 -0.044463556 0.13454443 -0.15619212 0.36371058 0.19405365 -0.029257724 -0.15627523 -0.082211226 -0.031812273 -0.10615006 0.27534664 0.025077324 0.13117166 -0.05643486 0.06328256 0.3567943 -0.60168743 0.3005646 -0.06676594 0.67715764 -0.10428462 0.5551278 0.4930658 -0.31621742 -0.51785886 -0.33509076 0.12771018 0.0515596 -0.1275401 0.07208549 -0.5823938 -0.07416169 -0.20847924 0.23654462 0.029749628 0.031177538 -0.06401348 -0.24399379 -0.30637574 0.19638896 0.06759672 0.28868788 0.20348938 -0.028657654 -0.13123809 -0.079314396 0.12952851 -0.19719328 0.031414893 0.010050677 -0.09300732 -0.28583887 0.09887549 0.132998 -0.040683456 0.007460608 0.24571793 -0.0713698 -0.2693695 0.34571066 0.23958175 0.13824527 -0.08303472 -0.19744188 0.07680438 -0.23854585 -0.061413065 -0.31553987 0.08246057 -0.41110703 0.06281182 0.05927142 0.09580122 0.39299408 0.26735336 -0.055083454 -0.063013315 0.011592129 0.036399495 -0.024802687 0.15639341 -0.36615363 0.045343425 0.2315806\n",
            "a 0.024971297 0.28144166 0.021699682 -0.12859876 -0.08717693 -0.18836609 -0.1123193 0.09564753 -0.101810284 0.1269952 0.0053809895 -0.13390514 -0.0016651338 -0.23811649 0.053298865 -0.0032377993 0.0133952545 0.32242095 -0.03230929 0.2409578 -0.048262373 -0.15489426 -0.018927176 -0.15098694 -0.062393676 0.34685338 -0.13145041 -0.07574895 -0.13631465 -0.11629781 0.2521962 -0.0969662 -0.15216482 0.034870826 -0.008844314 0.31327733 -0.24255557 0.1805833 0.39847612 -0.150411 0.04142175 0.09635181 -0.16194001 0.04817696 0.19396174 -0.086818926 -0.084666155 -0.30692425 0.13909256 0.19977136 -0.016480295 0.044157628 0.06537417 -0.1069289 -0.23336059 -0.06798998 0.06959626 0.026058545 0.1478664 0.3786073 0.23826513 -0.12144541 -0.28697807 0.20574236 0.18288757 -0.25232857 -0.27303407 -0.4274863 0.21721542 -0.0070845084 0.1413225 -0.13460186 -0.058833063 0.19702372 -0.15114135 -0.11105389 0.20677155 -0.2444753 0.13945906 -0.16042425 -0.12127341 0.07206597 0.086983085 0.059632912 -0.07206132 -0.11756713 0.15331507 -0.25099155 0.038682595 0.16577603 -0.013561051 -0.14778157 0.1607837 -0.39077157 0.14936481 0.33869562 0.36473003 -0.27796566 -0.068441756 0.19609272\n",
            "s 0.20675032 0.0075452463 -0.07225026 0.22454281 0.020655662 0.13041453 0.2954405 0.1005501 0.045936774 0.07289542 0.0778754 -0.29059142 0.07680573 -0.2625462 -0.002170807 0.010742905 0.10368325 0.08622686 -0.14339176 -0.057866566 -0.021268504 0.085788466 -0.08438631 0.081815146 -0.11685329 0.008105482 -0.21844171 -0.059849013 0.12011574 -0.009055843 -0.08701855 -0.052313384 -0.030176781 -0.09214335 0.17787191 0.01970121 0.21480852 -0.004414342 -0.025198918 0.052308537 0.12960467 -0.031527508 -0.19603944 -0.086688355 0.060097143 0.046157304 -0.101712815 -0.094538555 0.088672236 -0.09187481 -0.0017828582 -0.005966608 0.095684215 0.051780358 -0.06510921 -0.094526954 0.16614617 0.20248196 -0.00020978831 -0.2010738 -0.06985505 0.0516525 -0.30458853 0.008737699 0.044873185 -0.16885185 0.17001677 -0.027417483 -0.32118484 -0.1779165 -0.2819916 0.09089088 0.0962811 -0.09935873 -0.015493731 -0.045800123 -0.12707385 0.10935344 0.18478313 -0.036078293 0.064296655 0.16202314 -0.19492869 0.26176557 -0.15838802 -0.039528172 0.23906456 -0.13017221 0.16841216 -0.15447024 0.07627744 -0.114795506 0.012581228 0.008039164 0.10599043 -0.0153672 -0.0912405 0.23466896 0.09269285 -0.20659052\n",
            "t -0.11510009 -0.31364426 -0.0977772 0.2879289 -0.12673095 -0.17497912 0.5055338 0.15502444 -0.19897667 0.2366496 -0.10689887 0.062016446 0.17737381 -0.112177484 -0.15324461 0.06898287 -0.03702303 0.025142772 -0.28266767 -0.26362297 -0.17070565 -0.06226652 -0.25593832 -0.16089234 0.0302147 -0.34392917 -0.017198367 0.24589957 0.040922374 0.28861862 0.03205969 -0.08589966 0.019958587 -0.070050634 0.116668135 0.0758315 0.13170977 -0.1492541 -0.17340633 0.1965957 -0.09289687 -0.19481029 0.08588366 -0.09540539 0.06826657 0.083436474 0.26930907 0.038807467 -0.13393427 0.12969719 0.19595215 -0.0012280144 0.120415166 -0.070418745 -0.106153615 0.07449184 0.24912123 -0.094584465 -0.15061694 -0.0082552 0.102326274 0.05206792 0.2782834 -0.015260251 -0.059078638 0.382895 0.17558499 0.054931335 -0.011196272 -0.30547866 -0.0059580337 -0.046965186 -0.080083884 0.045279413 -0.21648327 0.22256114 -0.2362491 -0.08585175 0.2668444 0.026843479 -0.21701457 0.22504091 -0.106412694 0.03978995 -0.20229328 0.023392456 -0.24149849 -0.18814874 0.19390926 -0.18337461 0.012100868 0.088798165 -0.27081737 0.06869683 0.30710632 0.2206461 0.00817019 0.37735486 -0.011176822 0.09426995\n",
            "n 0.18528625 -0.29155213 -0.19985043 0.05702456 -0.19857363 0.15693173 0.34281325 0.14791796 0.19547386 -0.02210293 0.16105345 -0.30519438 0.4792621 0.17347611 0.17199141 0.2169705 0.042177737 0.060379 0.14998856 0.11903799 0.3269091 -0.01437091 0.049438156 -0.06185638 -0.22468069 -0.06541775 0.107413515 0.013982366 -0.12360808 -0.4164156 -0.037812468 -0.016869504 0.100541435 0.04585849 -0.14422047 0.009742075 0.1219671 0.025672596 0.03255965 -0.14001408 -0.36780643 0.04482021 -0.03841 -0.19029103 0.3433106 -0.059458014 0.12212264 -0.16425712 0.1413784 -0.40813693 0.07243837 0.00515489 -0.17885539 -0.0068410304 -0.40500197 0.34518886 0.048494488 -0.123603135 0.045041963 0.04178494 -0.44090956 -0.20132947 0.2901798 -0.64068097 0.059546385 -0.09016719 -0.12958747 0.17466547 0.14597052 0.03121468 -0.0004708952 0.07023485 -0.115658194 -0.0075054266 0.069665805 -0.19478169 0.07944381 0.3017834 -0.009453258 0.38834846 0.09660878 -0.12793227 -0.14348574 0.052021507 -0.06446571 0.19066119 -0.06414768 -0.11532929 -0.05904009 -0.30661064 -0.17894404 -0.10579505 0.014010884 0.19839628 -0.03266133 -0.32000217 0.06653656 0.15861423 0.12205209 0.06336303\n",
            "r 0.12709594 -0.03287267 -0.30522314 -0.057905417 -0.15180564 -0.21386217 -0.1563647 -0.23137832 0.059690554 -0.054086655 -0.4262976 -0.110501185 0.01015599 -0.006344415 0.19361955 -0.2212169 -0.05267109 0.13840188 -0.27264547 0.22081268 0.11461311 0.21588963 0.2434277 -0.3156291 -0.21254376 -0.0741493 -0.2842305 -0.25609982 0.17846508 0.15364316 0.022167072 -0.14099798 -0.1635435 -0.023298422 -0.11253625 0.15043312 -0.081752196 0.13355035 0.21272363 0.06274686 -0.13040468 0.12061162 -0.031675 -0.15564848 0.026300112 -0.42423052 0.02070163 0.10493591 0.19963479 -0.50791734 0.050662898 -0.12537701 -0.1342508 -0.3310645 -0.15251064 -0.099009395 -0.105495326 -0.22787009 0.18461826 -0.09811413 -0.14003672 0.30562297 -0.0812937 -0.32015288 -0.021143215 -0.15808359 0.030071137 0.22128652 0.058542427 0.03784791 -0.23465465 0.039945625 0.0048700836 0.18677968 -0.17248839 0.3250834 -0.35400337 0.05562486 0.20193566 0.3109877 0.1597248 -0.18355313 -0.14063017 0.052086268 0.10479616 -0.089483626 -0.023057695 -0.057851087 -0.04475482 -0.18705304 -0.16541487 -0.4688234 0.055901702 0.10554579 0.14373091 -0.22373737 0.19431473 0.13124755 -0.07833935 -0.035611566\n",
            "o 0.10296471 0.094866134 -0.06658296 0.03450942 -0.045301788 -0.2639174 -0.030022759 0.07558799 -0.2061115 0.12523776 -0.12670518 -0.17787336 -0.09465478 -0.20440778 0.18181989 -0.06014472 -0.13098916 -0.09479884 -0.005685602 0.3225842 -0.05422384 -0.1869515 -0.12903807 -0.31073418 -0.059405103 0.65183854 -0.11165048 -0.3167832 -0.3336969 -0.073704004 0.36190996 0.063201524 0.0023399359 0.086012244 -0.08991483 0.33543545 -0.31259724 0.18100493 0.40097007 -0.27425596 -0.045009598 -0.1904106 0.14957708 0.002065974 0.1380977 -0.1389205 -0.0935681 0.026001304 0.067546144 0.0696304 0.29727525 -0.012063442 -0.06553124 -0.13287626 -0.36169988 -0.086240076 -0.070299655 0.17266499 0.03564148 0.027628709 0.04656099 -0.29490194 -0.061628617 0.029371083 -0.087339364 -0.115060575 -0.020905051 -0.07469378 0.004172894 -0.1070053 0.08889793 0.16137771 0.032585282 0.06028299 0.008666856 -0.20570554 0.14500514 0.110998385 -0.00073509343 -0.11848449 -0.046680357 0.04257612 0.093501486 0.034860693 -0.14582933 0.09538704 0.14978242 -0.10376858 -0.02304055 0.069075845 0.08818657 -0.10428721 -0.19436432 -0.11179612 -0.19764668 0.120836146 -0.03539918 0.12719679 0.14240791 0.17867349\n",
            "l 0.10991201 -0.20808703 -0.10258818 -0.06014101 0.025969693 -0.14375307 0.118629724 -0.27664265 0.31323925 -0.27363512 -0.22015806 -0.162852 -0.052859545 0.1295603 -0.26956916 -0.38122088 0.019860169 0.444033 0.025666546 -0.06825943 0.18379724 0.09007954 0.4126639 -0.26889402 0.14753915 0.006833784 -0.10150373 -0.0354924 0.03255762 -0.07347697 -0.15971522 -0.045948807 -0.16275555 0.2302422 0.27443817 0.3065305 -0.17439197 -0.25562063 0.02290737 -0.09536206 0.16165578 0.30685025 -0.37117162 0.005574984 0.07003934 -0.2647411 0.1013448 -0.11300897 0.13303684 -0.24048126 -0.031400476 -0.18099257 -0.27375478 -0.24512458 -0.14812237 0.031394865 0.13494663 -0.05887758 -0.17744917 0.126137 0.065218024 -0.041103948 -0.18223515 -0.2149064 -0.005960731 0.13761362 -0.055492584 -0.34401774 0.10975953 -0.15864648 0.115100875 0.098223284 0.0074863946 -0.09706143 -0.11667159 0.17148226 -0.3459597 0.005205265 -0.018986845 0.00055211486 -0.19751954 0.15031186 -0.095104136 0.1954725 -0.12105542 0.20001891 0.058598075 -0.00074414787 0.08216073 -0.51507753 -0.3166288 0.14199339 0.19085681 -0.0651011 0.05870414 -0.036839046 0.11875201 0.39118785 0.123260595 -0.4216948\n",
            "c -0.12862396 -0.09658569 -0.20814784 0.31391242 0.14940223 0.16672872 0.51675266 -0.21936034 -0.099543236 0.14736441 0.06614421 0.17937347 0.19316454 -0.090254866 -0.33671308 0.13905342 0.05829013 0.2437155 -0.2602493 0.10651965 -0.20288847 0.034960575 -0.08799418 0.13956887 -0.020241922 -0.38050205 0.020723846 -0.09540129 0.41006935 0.24227755 -0.026598128 -0.11882748 -0.0067323335 -0.07496498 0.0517258 -0.16498147 0.37844762 -0.22025993 -0.1886063 0.42739803 -0.003363277 0.22574405 -0.34791616 -0.15440874 -0.07400662 0.025610212 0.25324267 -0.09272929 0.21212842 -0.1622476 -0.107548214 -0.022921905 0.30589035 0.22605827 0.042006955 -0.090118945 0.26289096 -0.18229283 0.06071868 -0.012777897 0.23369332 0.41617295 -0.15756573 0.1009934 0.089628145 0.2377554 0.07837039 0.050473824 -0.09505758 -0.15377554 -0.0065970467 -0.21459486 0.18486057 0.017713651 -0.020919804 0.088806465 -0.261364 -0.2704041 0.06372047 -0.02912748 -0.0055494388 0.13385744 -0.065329984 0.28513992 0.09655936 -0.28082505 0.01923018 0.03678382 0.21468794 0.31863663 0.26986355 -0.16523169 0.45029166 -0.07109649 0.2843308 0.023571013 0.14525023 -0.10689548 -0.2387873 0.11261016\n",
            "d 0.06629029 -0.17200111 -0.088591546 0.25016016 -0.11883441 -0.14165132 0.0017567491 0.39225498 0.10618845 0.06884233 -0.15798901 -0.2405332 0.11981488 -0.044694606 -0.029543364 0.09557421 0.0006475521 -0.062080882 -0.047667205 -0.1111359 -0.20967469 0.06412111 0.086099885 0.033764306 0.114861764 -0.23596299 -0.19520038 0.1406812 0.100063056 0.10398477 -0.18092161 -0.069148585 0.07496478 0.04624944 0.1254904 0.1537893 -0.078357324 -0.0606782 0.079135895 -0.1566193 0.1929079 -0.05680841 0.046883274 -0.13647996 -0.00999329 0.06317506 0.037192985 0.31788874 -0.23077519 -0.038377468 0.36968708 -0.2642482 0.01856588 -0.06989915 -0.09070713 -0.14700764 0.079333305 0.13734384 0.09029615 -0.11429111 0.14217629 0.073392905 -0.23477271 0.056906197 0.100357234 0.055053033 0.18708614 0.12210945 -0.29769096 -0.1776235 -0.09903542 0.09158902 0.0648183 0.116139926 0.076515436 0.008748708 -0.26717806 -0.0780145 0.010517111 0.055791404 0.34513372 -0.01302895 0.0039573484 -0.0027726826 -0.10684377 0.11187765 -0.07273744 -0.31096205 0.14603692 -0.31922042 -0.087775774 0.19775376 -0.03819563 -0.0011479182 0.03964883 0.14253567 -0.10699751 0.19920148 -0.06676598 -0.20939133\n",
            "m 0.16240512 -0.15226975 -0.41596386 0.11278309 0.18602055 0.17691997 0.05378958 -0.16693169 0.108847186 -0.14123744 -0.2693496 -0.021719059 0.18914342 0.14665152 -0.033683065 -0.085690156 -0.06762413 0.32353252 -0.12801066 0.006976275 -0.07126875 0.22246416 0.060552657 -0.08974003 0.087131836 -0.39446816 0.0006005393 0.0069196583 0.18204284 -0.023087068 -0.24224669 -0.15500474 -0.059652448 -0.013254742 -0.13012493 0.14036511 0.14996155 -0.026658485 0.033053726 0.31726152 -0.16013213 -0.026620902 0.1536764 -0.17992166 -0.015997075 0.0013133581 0.08747917 -0.009358527 0.1390413 -0.1288342 -0.060347967 -0.009781247 0.26187667 -0.11644787 -0.004875637 -0.103869215 -0.02663913 -0.17583111 -0.11542623 0.020068614 0.20458221 0.23366329 0.08074668 0.087655455 -0.023883509 0.3525971 -0.028946435 0.44607154 0.019041892 -0.036329173 0.050590582 -0.09936827 0.14823818 -0.074820645 0.06975369 -0.27555057 -0.29103264 -0.104034066 -0.32382327 0.17367473 0.21500069 -0.008562622 -0.1473543 0.07164775 0.024037862 -0.01449774 -0.0077855573 -0.013296881 -0.040835723 -0.19280113 -0.009239002 0.10985357 0.47945726 0.09403533 0.24117856 -0.2175014 0.056900986 0.116477 -0.17021437 0.10453824\n",
            "u -0.016258672 0.06803864 -0.69691914 -0.2742845 0.1866373 0.05964474 0.1361479 0.01059424 -0.028491719 0.2253674 0.26524597 0.3513212 0.009802029 -0.12870643 0.20795351 -0.16939546 -0.21322185 0.65056306 -0.0384838 0.086100124 0.09251844 0.08123856 -0.30442846 -0.13677171 0.31770793 0.35430783 0.34445217 0.3215928 -0.09848585 -0.080359265 0.19757256 0.5318116 -0.46779314 -0.0693233 0.027312102 -0.50536996 0.05707407 -0.24723862 -0.18979566 -0.0028010283 -0.29378408 -0.10532149 -0.6344797 0.00013487622 0.3485171 0.39654833 -0.3278588 -0.25970656 -0.08381237 0.32433352 -0.38910535 0.40084347 -0.16343531 0.15885542 -0.17003217 0.42189017 0.33280736 0.17150567 -0.09688512 -0.2374688 -0.38220188 0.16837032 0.35692036 0.03622373 0.08004309 -0.060392328 -0.014391368 -0.83811766 0.056502175 -0.34314045 -0.21556878 0.293693 0.037551247 -0.07486006 -0.35858098 0.114412434 0.35752454 0.2038143 0.25012687 -0.049197454 -0.37618622 0.18340738 -0.289833 0.011566708 -0.090343826 -0.3085476 -0.010651075 0.059628338 0.072678044 -0.10444407 -0.33647847 -0.30559847 -0.2586358 -0.08977741 0.2829444 0.10857938 0.44321144 -0.28625947 0.21579355 -0.20003018\n",
            "g 0.028186044 -0.022564296 -0.1570198 0.13083059 0.09141364 -0.16293085 0.40997085 0.08598956 -0.27747968 0.34847423 0.3429581 0.16256444 0.076608546 -0.060203873 -0.07286605 0.53525317 0.13050129 -0.33774015 -0.075288035 -0.27317375 0.052305162 0.36636022 -0.12251034 -0.48254684 -0.0020173932 0.020395186 -0.020267382 0.025905091 0.47531548 0.0012665874 -0.4776042 0.16152357 0.0062525584 -0.09003273 0.47623777 0.10484771 0.04082581 -0.26590317 -0.25706086 -0.1327514 0.046430778 -0.1678618 -0.19081713 -0.07169101 -0.29537138 -0.5186704 -0.20476466 -0.12666072 -0.15131424 0.06173216 -0.3703357 0.13805315 0.029957974 -0.33763382 0.4718309 -0.30680364 0.32429883 0.0004944815 0.06971688 0.008177613 -0.017303484 0.3157553 -0.35037702 -0.26535487 -0.3105849 0.294004 0.4376992 0.41376752 -0.2688075 -0.14504105 -0.38095993 0.15618782 0.12798938 -0.053916376 -0.090395615 -0.22793628 0.04959685 0.0640145 -0.19315241 -0.37146115 0.325451 0.28616607 -0.112220086 -0.31517538 -0.4047022 -0.09097435 -0.026945397 -0.25018865 0.028767083 -0.13233782 0.3912996 0.49408635 -0.09474019 0.0017701106 0.09285357 0.32366952 -0.13726309 -0.016339866 -0.03705388 0.4465442\n",
            "h -0.42341036 -0.26008186 -0.2902408 0.16446793 -0.3251925 0.17496121 -0.115100056 0.071069844 -0.012510812 -0.03347121 0.22880754 0.07210719 0.36863977 0.4297743 0.26415515 0.12755696 -0.33139583 -0.3147684 0.30049616 -0.13491413 0.25861296 0.044559397 -0.0089193955 -0.06907689 -0.29136297 -0.10718458 -0.08339399 0.15796238 0.09521953 -0.082371905 -0.2046673 -0.7144173 0.18358321 -0.070372805 -0.18058619 0.1407871 -0.0038977857 0.17829429 0.13178296 -0.291338 -0.33904415 -0.20782126 0.3653786 -0.106822446 0.09744172 -0.3608997 0.18600759 0.15325591 -0.21817808 -0.2830016 0.47649986 -0.091085725 -0.012365669 0.07298884 -0.32907194 0.046765048 -0.26480713 -0.10092865 0.08614612 0.25286648 0.037517283 -0.3477494 0.09667002 -0.47927094 0.1946705 -0.49779388 0.17035784 0.06834817 0.07809528 -0.18459563 -0.08465786 0.20092584 0.02590809 0.07765115 -0.14195587 -0.10601705 0.18737434 0.16292638 0.50839025 0.14366125 -0.057397205 -0.08495311 0.043296773 -0.01351443 -0.21379255 0.2787498 0.13339052 -0.2530833 -0.23986758 -0.15003431 -0.10327958 -0.57311136 -0.17298652 -0.11746723 -0.15666807 0.045900784 -0.039392073 0.17192034 0.034670316 0.1740363\n",
            "p -0.33940518 -0.42268112 0.014711153 -0.03630244 -0.37388238 -0.077709354 0.31435117 -0.2502759 -0.17107992 -0.22354892 -0.24364826 -0.066206366 0.22340612 0.04782271 -0.32986426 0.36057425 -0.041231 0.2146794 -0.08860197 -0.025851192 -0.37360495 -0.28498358 -0.06285492 -0.047208585 0.057277754 -0.6224713 -0.04445391 0.24249163 -0.1097311 -0.06411035 0.011313914 -0.51947254 0.5870395 -0.46234843 -0.12815687 -0.051802047 0.27484295 -0.30242512 -0.14135018 0.44399437 0.016119935 0.05602042 -0.0759589 -0.17509 -0.02681197 0.033194415 0.019366916 -0.17372973 0.18185765 0.01701014 0.059356604 -0.09289007 0.4669954 0.24818122 -0.002211541 -0.15399559 0.20354685 -0.3873061 0.13255827 0.10204229 0.08105478 0.36205816 0.020436458 0.042958673 0.18557994 0.067746624 0.04376442 -0.040994897 -0.06267212 -0.13366508 0.031379797 -0.32291222 0.10529016 -0.06984024 -0.10213408 0.009835487 -0.13447325 -0.12607737 0.12734053 -0.020042783 -0.11690801 0.083600186 -0.00110384 -0.029796544 0.3043603 0.01207463 0.14234923 0.02746168 -0.060351416 0.20179076 0.04636313 -0.24576396 0.03317839 0.16638285 0.2503366 0.009683258 0.20095411 0.14000878 -0.03565496 0.035717264\n",
            "y 0.047318876 -0.35082942 -0.3535022 0.24314198 0.011306533 0.4487079 0.06863733 -0.1396255 0.13177349 -0.18632048 0.1473747 0.07614022 0.25697476 0.14997813 -0.36350417 0.15083107 0.027565192 0.09524809 0.27442768 0.34666485 -0.26126996 0.14039679 0.06012454 0.27714536 0.29007545 0.21392776 0.276417 -0.48795244 0.08220657 0.13631538 0.13564324 0.19577806 0.04359515 -0.057557147 -0.2026665 -0.028680654 -0.07915886 -0.15979028 0.058884177 -0.09192583 -6.372512e-05 -0.4406047 0.32075652 -0.17585424 -0.06567593 0.36949185 -0.13166356 0.51898485 -0.15609829 -0.0019339555 0.3786795 -0.21370575 -0.07094275 0.05138444 0.053733073 -0.14817028 -0.08818134 0.065207615 0.3510416 -0.4344557 0.2441617 0.08290705 -0.015756777 -0.024124619 0.2381006 -0.28663772 0.50420487 -0.396281 -0.41972047 -0.20962194 -0.2585683 -0.027112065 0.2867699 -0.16825767 0.20002222 0.016142856 -0.03641384 0.021995366 -0.012992665 -0.22603561 0.1397483 0.13915204 0.122183956 0.1646573 0.15694034 0.044556614 0.36096603 0.056161743 -0.05486804 0.10125398 0.14003722 0.07569872 0.12990902 0.05708297 0.15451846 0.020192549 -0.1717556 -0.43669105 -0.19380344 -0.24372987\n",
            "f 0.1910352 -0.06869967 -0.6703754 0.0028784177 0.22267756 -0.04909223 0.02018936 -0.18261258 -0.065108724 -0.11979883 0.24952035 -0.100639746 0.5245533 0.0644348 -0.0015575796 0.08547153 0.4102089 0.7354033 0.12787315 0.043264978 -0.21011928 0.7489829 -0.13479882 0.42853764 -0.039968986 -0.5441463 0.20832309 0.10121065 0.21955359 -0.5623862 -0.26693663 -0.36121622 -0.021365603 0.09004783 0.039936256 0.2629881 0.112676 -0.050522827 0.22420786 0.1642982 -0.11613024 -0.2568544 0.32629833 -0.26975214 -0.040759172 0.33584222 0.08381648 -0.22739701 0.15868628 -0.0550336 -0.0026439487 0.02268548 0.16152096 0.15880187 -0.20747265 -0.14286883 -0.2184192 0.21149607 -0.24815154 0.013177712 0.4709755 -0.32411304 -0.113252215 0.45573565 0.05488851 -0.05079137 0.11241128 -0.35728493 -0.09634442 -0.15133563 0.05794596 -0.08206437 0.17011711 -0.061670057 0.16142996 -0.11373592 0.047370818 0.021913663 -0.13659866 -0.16176026 -0.18641733 0.119718246 0.26820076 0.14011152 -0.13477632 0.2662935 0.1093542 -0.07405448 0.19728991 -0.13442628 0.084656924 0.43629056 0.12049975 -0.37571236 -0.17352998 0.33214378 -0.118517645 -0.11790547 0.081763566 0.002839541\n",
            "b -0.09786343 -0.19364682 -0.08987203 0.12880394 0.19130325 0.09499243 -0.014921067 0.12138767 0.1649469 -0.1282767 0.23833518 0.35213655 0.05043184 0.4429865 -0.56454045 0.3414132 -0.37393874 0.099013954 -0.027707377 0.31414163 -0.35401636 -0.48458242 0.3778993 0.091161974 0.43752694 0.07165981 0.19976652 0.17192964 -0.10426606 -0.08955594 -0.1779701 -0.08271829 0.2669209 0.034821473 -0.07767721 -0.46311888 -0.15151486 -0.36736163 -0.015113044 -0.15970008 -0.010585817 0.2726949 -0.22995445 -0.018824484 0.107610784 -0.064650096 0.29749182 0.19315419 0.034600306 -0.09706585 0.19081002 -0.027694846 -0.1663265 0.35650235 -0.27858576 0.2596147 -0.08204507 -0.13733578 0.083143115 0.19875543 0.0848747 -0.07755707 0.08559457 -0.038273487 0.36941743 -0.13242045 -0.13267614 -0.13224196 -0.14474085 -0.11682913 0.23543592 -0.04323104 0.150017 0.09901107 0.36462066 -0.7250973 0.05195843 -0.21355253 -0.47403193 0.1781248 0.34954017 0.008176929 0.1175103 0.017994054 0.19420677 0.16444391 0.39322865 -0.2290667 -0.15268107 -0.20425695 -0.47933632 -0.12694217 0.4382295 -0.0987796 -0.03712978 0.012999191 0.25064433 -0.028295722 -0.0040214485 -0.32856134\n",
            "w -0.7191392 -0.0060952543 0.008911979 0.41575718 0.2866987 0.2543354 0.56252545 -0.2564284 -0.013953534 0.21265104 -0.005223675 0.12561655 -0.16003981 -0.07169883 -0.036361914 0.12897295 0.035073154 -0.41199958 0.25895858 -0.39256126 0.07372589 0.015389454 -0.26455125 0.1142413 0.16785066 -0.17724645 -0.08137863 0.270272 0.27532837 0.3620315 -0.2945891 0.054850984 0.031034041 0.041840807 0.26578668 0.18015307 -0.14277406 -0.33920646 -0.07283214 -0.13796015 -0.08790227 -0.14023419 -0.5231181 0.006208519 0.037213486 -0.2582694 -0.03227419 0.0837018 -0.18745764 0.05964639 0.08918272 -0.11263344 -0.016996307 0.10320466 0.0385809 0.26144066 0.26716852 -0.014671853 0.09886592 0.2651266 0.20328602 -0.12869501 0.14111714 -0.39696163 0.22479811 0.36182868 0.046192855 0.12266009 0.012887729 -0.08768062 0.019363869 0.051396087 0.31586614 -0.39966735 0.1419708 -0.66596556 0.31984904 0.055754367 -0.43805113 -0.30610132 0.3710776 -0.017624632 -0.061327834 -0.11302848 -0.119819134 -0.16720779 0.14792728 -0.13879831 -0.59251356 0.01694762 -0.3596302 -0.29903165 0.83889073 -0.20542566 0.01954957 0.024073774 0.12828195 -0.21965754 -0.4083287 0.13243586\n",
            "v -0.08061853 0.070893645 -0.082828276 0.019360786 -0.006323551 -0.3063114 0.21694547 0.2068079 -0.121127866 0.07307562 -0.21024647 -0.5152801 0.14605169 -0.43233898 0.37152904 0.41381663 0.4502548 0.13121451 -0.250004 -0.38884774 -0.41282153 0.41028446 -0.35631663 0.21688868 -0.021459403 -0.7618596 -0.42669156 0.6786814 -0.07322824 -0.39463583 -0.38885444 -0.22277246 0.2886561 -0.6270492 0.15441588 0.12136755 -0.08457917 -0.01080862 -0.01747659 0.23377687 0.24694878 -0.026876897 -0.10620755 -0.25420865 0.37362182 0.44005895 -0.24980447 -0.15022694 -0.079868846 0.07101094 -0.03754529 -0.08071596 0.3231585 -0.013289203 0.029608991 0.075794004 0.0066249617 0.2509174 0.10276207 -0.26224932 -0.0495481 0.15290305 -0.03372877 0.30081952 0.060018927 0.13088839 0.14036737 0.5482662 -0.44928265 0.105431445 -0.4940491 -0.36739147 -0.10218434 0.008921741 0.24940394 -0.49918854 -0.21019873 0.13343513 -0.5239404 0.3176525 0.7089007 0.009769707 -0.11833195 -0.29180294 0.16934583 0.086477995 0.25510564 -0.524927 0.04309564 -0.6642594 -0.51217157 0.2759629 0.14366207 0.22340639 0.24091297 0.058727965 0.122373804 0.27320138 0.0132404165 -0.112407915\n",
            "k -0.15199223 0.021192536 -0.34456572 0.014641145 0.027723992 0.6122188 -0.441586 0.039859183 0.269244 -0.27558425 0.47595745 0.26047707 0.12913162 0.30533296 0.1572735 -0.34535137 -0.40294886 0.23650482 0.22057834 0.018668229 0.2661674 -0.04816378 0.13674828 -0.06503216 0.134853 0.5279662 0.002041361 -0.01869882 -0.2697419 0.07224144 0.32300055 -0.017517209 -0.07248795 -0.19335221 -0.2938385 -0.1281594 -0.5227034 0.26122472 0.057450302 -0.042064372 -0.052993912 -0.09122122 0.38494083 0.058593467 0.4244604 0.09832843 0.4746662 0.17354314 0.15307881 -0.20624952 0.19486792 0.064622685 -0.19057903 -0.21998648 0.026157612 0.14744721 -0.00586602 -0.17676993 -0.09447622 -0.06535192 0.091588065 -0.2875911 0.11367566 -0.14237835 -0.048510965 -0.010449425 0.30889824 0.26721796 -0.19312121 -0.393338 -0.28563443 -0.22569956 -0.12903383 0.068977736 0.0061514163 -0.7874846 0.25431043 -0.25865915 -0.25767165 0.20278187 -0.080115214 0.33599046 0.15589108 0.05816248 0.020547632 0.157029 0.434132 -0.48072913 -0.17240825 -0.37951893 -0.37968394 -0.08806804 0.46529943 -0.16929889 0.27218658 0.31160593 0.14754672 0.22376227 -0.07722813 0.09610211\n",
            "j -0.35883334 0.09188713 -0.2555462 0.07203673 0.042344917 0.5191241 -0.37892157 0.36166543 0.5741599 -0.22773317 0.040340856 0.039243665 -0.31841597 0.16359773 0.1253456 -0.5055264 -0.86820346 0.12329513 0.18549615 0.19001357 0.25616023 -0.7490542 0.30605444 0.12885399 0.2565266 0.21303219 -0.0972228 0.31683758 -0.053279255 0.4966833 0.31447807 0.1297776 -0.050932188 0.32890567 -0.033874344 -0.2925431 -0.20777361 0.12139265 0.05577543 -0.13893457 0.43501076 0.8833597 -0.7848346 0.26209003 -0.035439894 -0.09326621 0.5468041 0.22675966 0.15275022 -0.21369228 0.095477335 -0.08560795 0.23522237 0.38041356 0.09583373 -0.0910955 0.72409606 -0.038066156 -0.17338447 0.19082305 0.094268486 0.27819735 -0.8099503 0.108320944 0.092663184 0.6502056 -0.017697088 0.47808275 -0.4109926 -0.65969807 0.119968995 0.13728967 0.34966043 0.1063076 0.07739233 -0.9201608 0.31596655 -0.5916271 -0.54998887 0.08796718 0.3946862 -0.051488206 0.16468494 0.028902555 -0.013748321 -0.30688608 -0.064854175 -0.52056086 -0.034666616 0.06620959 -0.22188778 -0.16744986 1.1139911 -0.4479548 -0.1937906 0.35251036 0.05871426 0.14101481 -0.35576662 -0.11055903\n",
            "x 0.53376716 0.31860572 -0.22857112 -0.24791142 -0.5458373 -0.2055281 0.09074499 0.25331077 -0.022383902 -0.118611366 -0.6658254 -0.8795408 -0.1335206 -0.69435465 0.36198384 0.14312568 0.3158729 0.51116467 -0.90476155 0.13824326 -0.17060836 0.20984279 -0.06341875 -0.12031005 -0.46426806 -0.8990697 -0.5807127 0.25826067 0.45795026 -0.24180728 -0.16042 -0.5812272 0.024112059 -0.6448751 -0.19787312 0.055030137 0.12737252 0.19568907 0.16677988 0.66468835 0.08365162 0.091215536 -0.019923719 -0.16639885 -0.10377374 -0.10344106 -0.42099524 -0.2604999 0.4123636 -0.2509351 0.1593828 -0.21328318 0.35692084 -0.2872912 -0.3250175 -0.41412693 0.011042335 -0.039041404 0.34988257 -0.39819136 -0.13750534 0.46608746 -0.3189323 0.19500694 -0.019280568 -0.19055383 0.24071553 0.08422992 -0.36295578 -0.06003858 -0.44244114 -0.5920931 0.30533502 -0.04548293 -0.38572764 0.53672546 -0.6119634 0.16780613 0.29090342 -0.03980823 -0.10274199 -0.066850916 -0.17940427 -0.16049366 0.2940809 -0.090002604 0.629858 -0.063090794 0.19386612 -0.15989147 0.050836645 -0.52476954 -0.23171115 0.29881343 0.25318658 -0.025024679 0.26308668 0.55856574 0.22620584 -0.19162796\n",
            "z -0.086884 -0.085368335 -0.12730707 -0.23170441 -1.2046307 0.49576706 0.15573162 1.1878929 0.73454225 0.3907225 0.6649897 -0.09757736 0.9372866 0.06799139 -0.27716216 0.30451998 0.7766781 0.41067845 0.87994915 -0.08156056 0.8590048 0.04494283 0.8032956 0.26063168 -0.27443698 -0.13135967 0.27601764 0.07994342 0.60310477 -0.11791333 0.43829775 0.47238308 -0.36032185 0.46633792 -1.033718 -0.47510394 -0.80709845 -0.22805624 0.10050416 -0.45043474 0.2725428 0.609922 -0.21233994 -0.031653028 0.68333334 0.15971918 0.6200937 0.5209948 -0.06525527 0.22285563 0.54957813 -0.4612631 -0.19674183 -1.267489 -0.08526607 -0.61426055 0.40728256 -0.8835405 1.2503048 0.5480277 0.3003498 0.72720087 -0.40325665 0.0055824746 -0.109265305 0.15197115 -0.5442524 0.039330203 0.36240965 -0.27438086 0.73930943 -0.066302724 -0.4338236 1.4534674 0.4573188 -0.35858962 -0.57721734 -1.3084624 -0.32876733 0.5303286 0.5341201 -0.39350864 0.40979525 0.140099 0.25460956 -0.031519856 -0.7731289 -0.44121656 0.11220198 0.49296 0.42796293 0.73581123 0.40056106 -0.11601325 0.9107147 0.13045143 0.45823696 -0.91479725 -0.64492184 0.27422622\n",
            "q 1.2667481 -1.004702 -0.54446155 -0.19487545 -0.14403954 -1.1325216 0.87312144 0.78823906 -0.080253094 0.14876957 0.4750703 -0.016073253 0.32685435 -0.19812168 -1.6666358 0.77327037 0.805042 0.727063 -1.1091204 -0.8164851 -0.8927113 0.13853869 1.1016923 0.2414246 -0.25344637 -1.0370169 -0.3106643 0.43807852 0.6858434 -0.05195817 -1.0386796 -0.6109028 0.24343629 0.50870335 -0.06976524 -0.7704929 0.19086652 -0.0371383 0.614594 0.10800153 0.74305415 0.049574506 0.5163928 0.5162983 -1.3754536 -0.8735966 0.20818315 0.33582488 0.61724496 0.015166962 -0.11725204 0.5470965 -0.15584104 0.12670721 0.0096940445 -0.66875875 -0.17434683 -0.09318803 -0.5935985 0.050110545 -0.0068937712 -0.06493317 -0.8524103 1.1076386 -0.41135147 -0.48749962 -0.40711877 0.435003 -0.6722772 -0.41450852 0.18312848 0.83057755 0.49699774 0.43792868 0.2877332 -0.68247765 -0.0070300885 0.037918184 -0.24613208 0.22931454 0.23956418 -0.5833794 0.021131797 -0.30187488 -0.23825292 -0.0009688199 0.12117272 -0.18331248 -0.41264927 -0.16144116 -0.19129083 -0.3772874 -0.8302722 -0.0906106 -0.6045784 -0.079176396 -0.21758701 1.0262017 1.1430453 -1.4418976\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "t4fZe46dczdf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_embedding(filename):\n",
        "\t# load embedding into memory, skip first line\n",
        "\tfile = open(filename,'r')\n",
        "\tlines = file.readlines()[1:]\n",
        "\tfile.close()\n",
        "\t# create a map of words to vectors\n",
        "\tembedding = dict()\n",
        "\tfor line in lines:\n",
        "\t\tparts = line.split()\n",
        "\t\t# key is string word, value is numpy array for vector\n",
        "\t\tembedding[parts[0]] = asarray(parts[1:], dtype='float32')\n",
        "\treturn embedding\n",
        "\n",
        "# create a weight matrix for the Embedding layer from a loaded embedding\n",
        "def get_weight_matrix(embedding, vocab):\n",
        "\t# total vocabulary size plus 0 for unknown words\n",
        "\tvocab_size = len(vocab) + 1\n",
        "\t# define weight matrix dimensions with all 0\n",
        "\tweight_matrix = zeros((vocab_size, 100))\n",
        "\t# step vocab, store vectors using the Tokenizer's integer mapping\n",
        "\tfor word, i in vocab.items():\n",
        "\t\tweight_matrix[i] = embedding.get(word)\n",
        "\treturn weight_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZMVwNqcYdRbU",
        "colab_type": "code",
        "outputId": "f9aebb58-1183-4ed2-a388-e761e310dddb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        }
      },
      "cell_type": "code",
      "source": [
        "#load embedding from file\n",
        "raw_embedding = load_embedding('embedding_word2vec.txt')\n",
        "# get vectors in the right order\n",
        "embedding_vectors = get_weight_matrix(raw_embedding, tokenizer.word_index)\n",
        "# create the embedding layer\n",
        "embedding_layer = Embedding(vocab_size, 100, weights=[embedding_vectors], input_length=max_length, trainable=False)\n",
        "\n",
        "# define model\n",
        "model = Sequential()\n",
        "model.add(embedding_layer)\n",
        "model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "print(model.summary())\n",
        "# compile network\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# fit network\n",
        "model.fit(Xtrain, ytrain, epochs=10, verbose=2)\n",
        "# evaluate\n",
        "loss, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
        "print('Test Accuracy: %f' % (acc*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_6 (Embedding)      (None, 1317, 100)         2576800   \n",
            "_________________________________________________________________\n",
            "conv1d_6 (Conv1D)            (None, 1313, 128)         64128     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_4 (MaxPooling1 (None, 656, 128)          0         \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 83968)             0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 1)                 83969     \n",
            "=================================================================\n",
            "Total params: 2,724,897\n",
            "Trainable params: 148,097\n",
            "Non-trainable params: 2,576,800\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 17s - loss: 8.0942 - acc: 0.4978\n",
            "Epoch 2/10\n",
            " - 17s - loss: 8.0590 - acc: 0.5000\n",
            "Epoch 3/10\n",
            " - 17s - loss: 8.0590 - acc: 0.5000\n",
            "Epoch 4/10\n",
            " - 17s - loss: 8.0590 - acc: 0.5000\n",
            "Epoch 5/10\n",
            " - 17s - loss: 8.0590 - acc: 0.5000\n",
            "Epoch 6/10\n",
            " - 17s - loss: 8.0590 - acc: 0.5000\n",
            "Epoch 7/10\n",
            " - 17s - loss: 8.0590 - acc: 0.5000\n",
            "Epoch 8/10\n",
            " - 17s - loss: 8.0590 - acc: 0.5000\n",
            "Epoch 9/10\n",
            " - 17s - loss: 8.0590 - acc: 0.5000\n",
            "Epoch 10/10\n",
            " - 17s - loss: 8.0590 - acc: 0.5000\n",
            "Test Accuracy: 50.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "R5a1LptLeh0U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Try Glove embeddings"
      ]
    },
    {
      "metadata": {
        "id": "KWSn9Llad19l",
        "colab_type": "code",
        "outputId": "c13c2085-e0a2-433d-f389-3028c8fdfde7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        }
      },
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove*.zip "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-11-06 19:23:20--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2018-11-06 19:23:20--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  97.1MB/s    in 8.8s    \n",
            "\n",
            "2018-11-06 19:23:29 (93.4 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ieGPy6ULiXcu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# load the vocabulary\n",
        "vocab_filename = 'vocab_movie.txt'\n",
        "vocab = load_doc(vocab_filename)\n",
        "vocab = vocab.split()\n",
        "vocab = set(vocab)\n",
        "\n",
        "# load all training reviews\n",
        "train_positive_docs = process_docs('txt_sentoken/pos', vocab, True)\n",
        "train_negative_docs = process_docs('txt_sentoken/neg', vocab, True)\n",
        "train_docs = train_negative_docs + train_positive_docs\n",
        "\n",
        "# create the tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_docs)\n",
        "test_encoded_docs = tokenizer.texts_to_sequences(train_docs)\n",
        "max_length = max([len(s.split()) for s in train_docs])\n",
        "Xtrain = pad_sequences(test_encoded_docs, maxlen=max_length, padding='post')\n",
        "ytrain = array([0 for _ in range(len(train_negative_docs))] + [1 for _ in range(len(train_positive_docs))])\n",
        "\n",
        "\n",
        "# load all test reviews\n",
        "test_positive_docs = process_docs('txt_sentoken/pos', vocab, False)\n",
        "test_negative_docs = process_docs('txt_sentoken/neg', vocab, False)\n",
        "test_docs = test_negative_docs + test_positive_docs\n",
        "test_encoded_docs = tokenizer.texts_to_sequences(test_docs) #reuse the tokens from training set\n",
        "Xtest = pad_sequences(test_encoded_docs, maxlen=max_length, padding='post')\n",
        "ytest = array([0 for _ in range(len(test_negative_docs))] + [1 for _ in range(len(test_positive_docs))])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9jFqz0dyepP0",
        "colab_type": "code",
        "outputId": "c37874f1-c131-4fb2-f994-15b5a069ee78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        }
      },
      "cell_type": "code",
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1\n",
        " \n",
        "# load embedding from file\n",
        "raw_embedding = load_embedding('glove.6B.100d.txt')\n",
        "embedding_vectors = get_weight_matrix(raw_embedding, tokenizer.word_index)\n",
        "embedding_layer = Embedding(vocab_size, 100, weights=[embedding_vectors], input_length=max_length, trainable=False)\n",
        " \n",
        "# define model\n",
        "model = Sequential()\n",
        "model.add(embedding_layer)\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# compile network\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(Xtrain, ytrain, epochs=10, verbose=2)\n",
        "\n",
        "# evaluate\n",
        "loss, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
        "print('Test Accuracy: %f' % (acc*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_14 (Embedding)     (None, 1317, 100)         2576800   \n",
            "_________________________________________________________________\n",
            "dropout_17 (Dropout)         (None, 1317, 100)         0         \n",
            "_________________________________________________________________\n",
            "conv1d_14 (Conv1D)           (None, 1313, 128)         64128     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_12 (MaxPooling (None, 656, 128)          0         \n",
            "_________________________________________________________________\n",
            "dropout_18 (Dropout)         (None, 656, 128)          0         \n",
            "_________________________________________________________________\n",
            "flatten_12 (Flatten)         (None, 83968)             0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 1)                 83969     \n",
            "=================================================================\n",
            "Total params: 2,724,897\n",
            "Trainable params: 148,097\n",
            "Non-trainable params: 2,576,800\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            " - 22s - loss: 0.8724 - acc: 0.5122\n",
            "Epoch 2/10\n",
            " - 21s - loss: 0.6269 - acc: 0.6794\n",
            "Epoch 3/10\n",
            " - 21s - loss: 0.4803 - acc: 0.7978\n",
            "Epoch 4/10\n",
            " - 21s - loss: 0.3122 - acc: 0.8989\n",
            "Epoch 5/10\n",
            " - 21s - loss: 0.1784 - acc: 0.9611\n",
            "Epoch 6/10\n",
            " - 21s - loss: 0.1247 - acc: 0.9683\n",
            "Epoch 7/10\n",
            " - 21s - loss: 0.0704 - acc: 0.9911\n",
            "Epoch 8/10\n",
            " - 21s - loss: 0.0487 - acc: 0.9956\n",
            "Epoch 9/10\n",
            " - 21s - loss: 0.0602 - acc: 0.9900\n",
            "Epoch 10/10\n",
            " - 21s - loss: 0.0342 - acc: 0.9967\n",
            "Test Accuracy: 75.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lMJIinSqfvnv",
        "colab_type": "code",
        "outputId": "0d0c486c-70c5-41f4-8b13-3dc98b563144",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "print (model.predict_classes(Xtest)[1])\n",
        "print (ytest[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0]\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qsh22QC4je2J",
        "colab_type": "code",
        "outputId": "2784b33f-b8c2-4d66-e815-da02d9462f05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "cell_type": "code",
      "source": [
        "train_docs[1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'following review contains spoilers please someone stop joel schumacher vomits film tries pass movie chuck dowling jacksonville film journal wrote sentence review batman robin apparently one heard words oh wish heard oh wish someone stopped hes done plot similar paul schrader film hardcore surprisingly decent engrossing tale fact everything opening minutes see private detective tom welles nicolas cage working case prestigious member government keep fact mind cage trusted important case solves returns home wife new daughter even finds time rake yard keep mind important well hes hired rich elderly woman solve mystery husbands death comes across film private safe appears snuff film sort urban legend pornography industry someone actually brutally murdered camera woman wants welles locate young girl film hopefully disprove fact snuff film sets journey sordid underworld sleaze discover truth film main problem whole film completely pointless welles actually selected case old womans lawyer anthony heald actor always plays type character bad guy first see first instinct hes behind whole thing choice casting film welles begins investigation times see shadowy figure following person could possibly lawyer someone sent lawyer one knows welles even investigating anything yet first instinct confirmed fact revealed lawyer part whole thing tells welles chosen case young inexperienced one ridiculous since going kill anyway tell whole thing moments earlier see welles hired case hes told hired comes highly recommended huh also lawyer reveals part whole thing main goal get film back give first place lawyer film whole time gave away private detective thinking would give case return film totally absurd screenwriter seven late film theres confrontation main characters struggle one characters guns ends underneath car cage handcuffed bed rapidly trying get gun table james character goes gun car supposed scene suspense see gun slightly reach side car hes trying reach instead going around car side easily reach keeps stretching go around side car goes minutes cage gets gun gandolfini decides would best go around side car boy schumacher really really getting nerves think final straw would cage returns home devastated everything hes seen case longer able rake lawn actually see shot pathetically poking leaves yard things bugged one would music call whenever score starts sounds like techno music awful indian middle eastern type music accompany awful decision joel another cages performance point hes capable wide range performances either good bad delivers wooden one shines hes interacting joaquin phoenix gives great performance porn shop clerk heart gold helps cage case youre capable much better mr cage mr schumacher im starting doubt youre capable anything else besides urinating onto institution american cinema'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "metadata": {
        "id": "W1ENKt-ojmMN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}