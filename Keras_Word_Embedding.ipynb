{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Keras_Word_Embedding.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"a-_BwacGBx4k","colab_type":"code","colab":{}},"cell_type":"code","source":["!pip install -q keras"],"execution_count":0,"outputs":[]},{"metadata":{"id":"FFlN_AxVB91k","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"4937dbb8-3a83-4369-fccc-b1aafab0c5ba","executionInfo":{"status":"ok","timestamp":1541525792306,"user_tz":480,"elapsed":1645,"user":{"displayName":"Xiwang Li","photoUrl":"","userId":"16306541610119186852"}}},"cell_type":"code","source":["import numpy as np\n","from numpy import array\n","from numpy import asarray\n","from numpy import zeros\n","from keras.preprocessing.text import one_hot\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import Flatten\n","from keras.layers import Embedding"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"metadata":{"id":"yPJKYL8XNhvQ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":830},"outputId":"0a63f3cc-fe26-41c7-e587-a4beef36e573","executionInfo":{"status":"ok","timestamp":1541525864203,"user_tz":480,"elapsed":11725,"user":{"displayName":"Xiwang Li","photoUrl":"","userId":"16306541610119186852"}}},"cell_type":"code","source":["!pip install --upgrade gensim"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Collecting gensim\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/a4/d10c0acc8528d838cda5eede0ee9c784caa598dbf40bd0911ff8d067a7eb/gensim-3.6.0-cp36-cp36m-manylinux1_x86_64.whl (23.6MB)\n","\u001b[K    100% |████████████████████████████████| 23.6MB 2.0MB/s \n","\u001b[?25hCollecting smart-open>=1.2.1 (from gensim)\n","  Downloading https://files.pythonhosted.org/packages/4b/1f/6f27e3682124de63ac97a0a5876da6186de6c19410feab66c1543afab055/smart_open-1.7.1.tar.gz\n","Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (0.19.1)\n","Requirement already satisfied, skipping upgrade: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.11.0)\n","Requirement already satisfied, skipping upgrade: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.14.6)\n","Collecting boto>=2.32 (from smart-open>=1.2.1->gensim)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/23/10/c0b78c27298029e4454a472a1919bde20cb182dab1662cec7f2ca1dcc523/boto-2.49.0-py2.py3-none-any.whl (1.4MB)\n","\u001b[K    100% |████████████████████████████████| 1.4MB 15.1MB/s \n","\u001b[?25hCollecting bz2file (from smart-open>=1.2.1->gensim)\n","  Downloading https://files.pythonhosted.org/packages/61/39/122222b5e85cd41c391b68a99ee296584b2a2d1d233e7ee32b4532384f2d/bz2file-0.98.tar.gz\n","Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.18.4)\n","Collecting boto3 (from smart-open>=1.2.1->gensim)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/77/ee/c97cb0954134e93c9ce7f0c5fabbd23b3d580036db024297c895bfb6ef28/boto3-1.9.38-py2.py3-none-any.whl (128kB)\n","\u001b[K    100% |████████████████████████████████| 133kB 26.9MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2018.10.15)\n","Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n","Requirement already satisfied, skipping upgrade: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.6)\n","Requirement already satisfied, skipping upgrade: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.22)\n","Collecting botocore<1.13.0,>=1.12.38 (from boto3->smart-open>=1.2.1->gensim)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5b/1c/02fe38b978498d8d466d748fe5513f6d6aa88fc28e21aec0b7dc679f6685/botocore-1.12.38-py2.py3-none-any.whl (4.7MB)\n","\u001b[K    100% |████████████████████████████████| 4.8MB 6.2MB/s \n","\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1 (from boto3->smart-open>=1.2.1->gensim)\n","  Downloading https://files.pythonhosted.org/packages/b7/31/05c8d001f7f87f0f07289a5fc0fc3832e9a57f2dbd4d3b0fee70e0d51365/jmespath-0.9.3-py2.py3-none-any.whl\n","Collecting s3transfer<0.2.0,>=0.1.10 (from boto3->smart-open>=1.2.1->gensim)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/14/2a0004d487464d120c9fb85313a75cd3d71a7506955be458eebfe19a6b1d/s3transfer-0.1.13-py2.py3-none-any.whl (59kB)\n","\u001b[K    100% |████████████████████████████████| 61kB 22.6MB/s \n","\u001b[?25hCollecting docutils>=0.10 (from botocore<1.13.0,>=1.12.38->boto3->smart-open>=1.2.1->gensim)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/fa/08e9e6e0e3cbd1d362c3bbee8d01d0aedb2155c4ac112b19ef3cae8eed8d/docutils-0.14-py3-none-any.whl (543kB)\n","\u001b[K    100% |████████████████████████████████| 552kB 24.4MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.38->boto3->smart-open>=1.2.1->gensim) (2.5.3)\n","Building wheels for collected packages: smart-open, bz2file\n","  Running setup.py bdist_wheel for smart-open ... \u001b[?25l-\b \b\\\b \bdone\n","\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/23/00/44/e5b939f7a80c04e32297dbd6d96fa3065af89ecf57e2b5f89f\n","  Running setup.py bdist_wheel for bz2file ... \u001b[?25l-\b \bdone\n","\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/81/75/d6/e1317bf09bf1af5a30befc2a007869fa6e1f516b8f7c591cb9\n","Successfully built smart-open bz2file\n","Installing collected packages: boto, bz2file, docutils, jmespath, botocore, s3transfer, boto3, smart-open, gensim\n","Successfully installed boto-2.49.0 boto3-1.9.38 botocore-1.12.38 bz2file-0.98 docutils-0.14 gensim-3.6.0 jmespath-0.9.3 s3transfer-0.1.13 smart-open-1.7.1\n"],"name":"stdout"}]},{"metadata":{"id":"9WJ-ZuPpO9EF","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":646},"outputId":"b26ea7e3-86ce-4f60-b7ef-33e6a2315609","executionInfo":{"status":"ok","timestamp":1541461543964,"user_tz":480,"elapsed":3007,"user":{"displayName":"Xiwang Li","photoUrl":"","userId":"16306541610119186852"}}},"cell_type":"code","source":["# define documents\n","docs = ['Well done!',\n","        'Good work',\n","        'Great effort',\n","        'nice work',\n","        'Excellent!',\n","        'Weak',\n","        'Poor effort!',\n","        'not good',\n","        'poor work',\n","        'Could have done better.', 'Very Good', 'can be better', 'very poor']\n","\n","# define class labels\n","labels = array([1,1,1,1,1,0,0,0,0,0,1,0,0])\n","\n","\n","# integer encode the documents\n","vocab_size = 50\n","encoded_docs = [one_hot(d, vocab_size) for d in docs]\n","print(encoded_docs, '\\n')\n","\n","# pad documents to a max length of 4 words\n","max_length = 4\n","padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n","print(padded_docs, '\\n')\n","\n","train_doc, train_label = padded_docs[:-3], labels[:-3]\n","\n","# define the model\n","model = Sequential()\n","model.add(Embedding(vocab_size, 8, input_length=max_length))\n","model.add(Flatten())\n","model.add(Dense(1, activation='sigmoid'))\n","# compile the model\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n","# summarize the model\n","print(model.summary(), '\\n')\n","# fit the model\n","model.fit(train_doc, train_label, epochs=50, verbose=0)\n","# evaluate the model\n","\n","loss, accuracy = model.evaluate(train_doc, train_label, verbose=0)\n","print('Accuracy: %f' % (accuracy*100), '\\n')\n","\n","doc_test = padded_docs[-3:]\n","print (docs[-3:])\n","ynew = model.predict_classes(doc_test)\n","pnew = model.predict_proba(doc_test)\n","for i in range(len(doc_test)):\n","    print(\"Y=%s, Predicted=%s\" % (ynew[i], pnew[i]))"],"execution_count":47,"outputs":[{"output_type":"stream","text":["[[39, 29], [5, 8], [4, 1], [24, 8], [35], [31], [32, 1], [25, 5], [32, 8], [8, 39, 29, 36], [34, 5], [16, 16, 36], [34, 32]] \n","\n","[[39 29  0  0]\n"," [ 5  8  0  0]\n"," [ 4  1  0  0]\n"," [24  8  0  0]\n"," [35  0  0  0]\n"," [31  0  0  0]\n"," [32  1  0  0]\n"," [25  5  0  0]\n"," [32  8  0  0]\n"," [ 8 39 29 36]\n"," [34  5  0  0]\n"," [16 16 36  0]\n"," [34 32  0  0]] \n","\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_12 (Embedding)     (None, 4, 8)              400       \n","_________________________________________________________________\n","flatten_7 (Flatten)          (None, 32)                0         \n","_________________________________________________________________\n","dense_13 (Dense)             (None, 1)                 33        \n","=================================================================\n","Total params: 433\n","Trainable params: 433\n","Non-trainable params: 0\n","_________________________________________________________________\n","None \n","\n","Accuracy: 80.000001 \n","\n","['Very Good', 'can be better', 'very poor']\n","Y=[0], Predicted=[0.4989309]\n","Y=[0], Predicted=[0.49827912]\n","Y=[1], Predicted=[0.51360536]\n"],"name":"stdout"}]},{"metadata":{"id":"kRRGtu7VDBy1","colab_type":"text"},"cell_type":"markdown","source":["# Word2Vec\n"]},{"metadata":{"id":"yQboRtPtCpSf","colab_type":"text"},"cell_type":"markdown","source":["Using Pre-Trained GloVe Embedding"]},{"metadata":{"id":"cYLM7khgCoQD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":388},"outputId":"d3bdf907-1c19-4e64-d24b-780f663ad04e","executionInfo":{"status":"ok","timestamp":1541458126049,"user_tz":480,"elapsed":54515,"user":{"displayName":"Xiwang Li","photoUrl":"","userId":"16306541610119186852"}}},"cell_type":"code","source":["!wget http://nlp.stanford.edu/data/glove.6B.zip\n","!unzip glove*.zip  "],"execution_count":8,"outputs":[{"output_type":"stream","text":["--2018-11-05 22:47:53--  http://nlp.stanford.edu/data/glove.6B.zip\n","Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n","Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n","--2018-11-05 22:47:54--  https://nlp.stanford.edu/data/glove.6B.zip\n","Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 862182613 (822M) [application/zip]\n","Saving to: ‘glove.6B.zip’\n","\n","glove.6B.zip        100%[===================>] 822.24M  23.7MB/s    in 29s     \n","\n","2018-11-05 22:48:22 (28.8 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n","\n","Archive:  glove.6B.zip\n","  inflating: glove.6B.50d.txt        \n","  inflating: glove.6B.100d.txt       \n","  inflating: glove.6B.200d.txt       \n","  inflating: glove.6B.300d.txt       \n"],"name":"stdout"}]},{"metadata":{"id":"FEl0HbIZCRq7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":99},"outputId":"16f4f006-308c-469b-9a2a-cbe00b0db899","executionInfo":{"status":"ok","timestamp":1541458130773,"user_tz":480,"elapsed":1933,"user":{"displayName":"Xiwang Li","photoUrl":"","userId":"16306541610119186852"}}},"cell_type":"code","source":["!ls\n","!pwd"],"execution_count":9,"outputs":[{"output_type":"stream","text":["glove.6B.100d.txt  glove.6B.300d.txt  glove.6B.zip\n","glove.6B.200d.txt  glove.6B.50d.txt   sample_data\n","/content\n"],"name":"stdout"}]},{"metadata":{"id":"ocyFwAbhCRfm","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":527},"outputId":"21ccd586-ae8f-4aa9-fd87-8a45a6359e25","executionInfo":{"status":"ok","timestamp":1541458172554,"user_tz":480,"elapsed":13823,"user":{"displayName":"Xiwang Li","photoUrl":"","userId":"16306541610119186852"}}},"cell_type":"code","source":["# define documents\n","docs = ['Well done!',\n","        'Good work',\n","        'Great effort',\n","        'nice work',\n","        'Excellent!',\n","        'Weak',\n","        'Poor effort!',\n","        'not good',\n","        'poor work',\n","        'Could have done better.', 'Good', 'can not be better', 'poor']\n","\n","# define class labels\n","labels = array([1,1,1,1,1,0,0,0,0,0,1,0,0])\n","# prepare tokenizer\n","t = Tokenizer()\n","t.fit_on_texts(docs)\n","vocab_size = len(t.word_index) + 1\n","# integer encode the documents\n","encoded_docs = t.texts_to_sequences(docs)\n","print(encoded_docs)\n","# pad documents to a max length of 4 words\n","max_length = 4\n","padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n","print(padded_docs)\n","\n","train_doc, train_label = padded_docs[:-3], labels[:-3]\n","\n","\n","\n","# load the whole embedding into memory\n","embeddings_index = dict()\n","f = open('glove.6B.100d.txt')\n","for line in f:\n","    values = line.split()\n","    word = values[0]\n","    coefs = asarray(values[1:], dtype='float32')\n","    embeddings_index[word] = coefs\n","f.close()\n","print('Loaded %s word vectors.' % len(embeddings_index))\n","# create a weight matrix for words in training docs\n","embedding_matrix = zeros((vocab_size, 100))\n","for word, i in t.word_index.items():\n","    embedding_vector = embeddings_index.get(word)\n","    if embedding_vector is not None:\n","        embedding_matrix[i] = embedding_vector\n","        \n","# define model\n","model = Sequential()\n","e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=4, trainable=False)\n","model.add(e)\n","model.add(Flatten())\n","model.add(Dense(1, activation='sigmoid'))\n","# compile the model\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n","# summarize the model\n","print(model.summary())\n","# fit the model\n","model.fit(train_doc, train_label, epochs=50, verbose=0)\n","# evaluate the model\n","loss, accuracy = model.evaluate(train_doc, train_label, verbose=0)\n","print('Accuracy: %f' % (accuracy*100))"],"execution_count":10,"outputs":[{"output_type":"stream","text":["[[8, 4], [1, 2], [9, 5], [10, 2], [11], [12], [3, 5], [6, 1], [3, 2], [13, 14, 4, 7], [1], [15, 6, 16, 7], [3]]\n","[[ 8  4  0  0]\n"," [ 1  2  0  0]\n"," [ 9  5  0  0]\n"," [10  2  0  0]\n"," [11  0  0  0]\n"," [12  0  0  0]\n"," [ 3  5  0  0]\n"," [ 6  1  0  0]\n"," [ 3  2  0  0]\n"," [13 14  4  7]\n"," [ 1  0  0  0]\n"," [15  6 16  7]\n"," [ 3  0  0  0]]\n","Loaded 400000 word vectors.\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_3 (Embedding)      (None, 4, 100)            1700      \n","_________________________________________________________________\n","flatten_1 (Flatten)          (None, 400)               0         \n","_________________________________________________________________\n","dense_3 (Dense)              (None, 1)                 401       \n","=================================================================\n","Total params: 2,101\n","Trainable params: 401\n","Non-trainable params: 1,700\n","_________________________________________________________________\n","None\n","Accuracy: 100.000000\n"],"name":"stdout"}]},{"metadata":{"id":"mqnIv_vKEZKS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"outputId":"3fd6da58-5a4a-466a-e1f0-efcdf01a0154","executionInfo":{"status":"ok","timestamp":1541458205806,"user_tz":480,"elapsed":346,"user":{"displayName":"Xiwang Li","photoUrl":"","userId":"16306541610119186852"}}},"cell_type":"code","source":["doc_test = padded_docs[-3:]\n","print (docs[-3:])\n","ynew = model.predict_classes(doc_test)\n","pnew = model.predict_proba(doc_test)\n","for i in range(len(doc_test)):\n","    print(\"Y=%s, Predicted=%s\" % (ynew[i], pnew[i]))"],"execution_count":11,"outputs":[{"output_type":"stream","text":["['Good', 'can not be better', 'poor']\n","Y=[1], Predicted=[0.64661646]\n","Y=[0], Predicted=[0.0947505]\n","Y=[0], Predicted=[0.37935227]\n"],"name":"stdout"}]},{"metadata":{"id":"t_gRLnIXCZxO","colab_type":"text"},"cell_type":"markdown","source":["# Sequence Classification with LSTM for IMDB\n","\n","Develope an LSTM model for sequence classification problem using the Keras IMDB dataset. \n","I also use Convolutional Neural Networks to capature the spatial relationships (5 or several words together)\n","\n","See the networks developed, the first layer is an embedding layer. It turns positive integers into dense vectors (embedding vectors) of fixed size , which can then be used by convolution layer. Eg. [[4], [20]] -> [[0.25, 0.1], [0.6, -0.2]] The input of embedding layer can only be positive integers, which can be provided by Tokenizer (see the \"moive_review\" example below). The Embedding layer is initialized with random weights, and will be trained from all the words in the training dataset.\n","\n","The Conv1D layer is an one-dimension convolution layer, which can capature the sequence of words (n-grams). \n","![](https://raw.githubusercontent.com/eisenjulian/nlp_estimator_tutorial/master/conv.png)\n","\n","      Soure: Learning to Rank Short Text Pairs with Convolutional Deep Neural Networks by Severyn et al. [2015]\n","\n","The general idea of LSTM is:\n","\n","![](https://cdn-images-1.medium.com/max/800/1*laH0_xXEkFE0lKJu54gkFQ.png)\n","\n","         source Understanding LSTM and its diagrams by Shi Yan \n","\n","Please see the \"Understanding LSTM and its diagrams\" by Shi Yan (https://medium.com/mlreview/understanding-lstm-and-its-diagrams-37e2f46f1714) and Adrew Ng's course for details about LSTM or GRU (https://www.coursera.org/lecture/nlp-sequence-models/long-short-term-memory-lstm-KXoay).\n"]},{"metadata":{"id":"PhsGR3QkCA2O","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":442},"outputId":"4a4ce06f-5b80-4727-f77d-d2cc9c785e76","executionInfo":{"status":"ok","timestamp":1541459391346,"user_tz":480,"elapsed":1073719,"user":{"displayName":"Xiwang Li","photoUrl":"","userId":"16306541610119186852"}}},"cell_type":"code","source":["from keras.datasets import imdb\n","from keras.preprocessing import sequence\n","from keras.layers import LSTM, Dropout\n","from keras.layers.convolutional import Conv1D, MaxPooling1D\n","\n","# fix random seed for reproducibility\n","np.random.seed(7)\n","# load the dataset but only keep the top n words, zero the rest\n","top_words = 5000\n","(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n","\n","#X_train, y_train = X_train[:int(len(X_train)/2)], y_train[:int(len(y_train)/2)]\n","# truncate and pad input sequences\n","max_review_length = 500\n","X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n","X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n","# create the model\n","embedding_vecor_length = 32\n","model = Sequential()\n","model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n","model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n","# num of parameter: num_filters * (kernel_size * embedding_vecor_length) + num_filters = 32*(3*32)+32=3104\n","model.add(MaxPooling1D(pool_size=2))\n","model.add(LSTM(100))\n","# num of parameter: 4(n*m + n^2 +n)= 4*(100*32 + 100.^2 + 100) = 53200\n","model.add(Dense(1, activation='sigmoid'))\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","print(model.summary())\n","model.fit(X_train, y_train, epochs=3, batch_size=64)\n","\n","\n","# Final evaluation of the model\n","scores = model.evaluate(X_test, y_test, verbose=0)\n","print(\"Accuracy: %.2f%%\" % (scores[1]*100))"],"execution_count":14,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_6 (Embedding)      (None, 500, 32)           160000    \n","_________________________________________________________________\n","conv1d_2 (Conv1D)            (None, 500, 32)           3104      \n","_________________________________________________________________\n","max_pooling1d_1 (MaxPooling1 (None, 250, 32)           0         \n","_________________________________________________________________\n","lstm_3 (LSTM)                (None, 100)               53200     \n","_________________________________________________________________\n","dense_4 (Dense)              (None, 1)                 101       \n","=================================================================\n","Total params: 216,405\n","Trainable params: 216,405\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n","Epoch 1/3\n","25000/25000 [==============================] - 285s 11ms/step - loss: 0.4363 - acc: 0.7847\n","Epoch 2/3\n","25000/25000 [==============================] - 285s 11ms/step - loss: 0.2483 - acc: 0.9040\n","Epoch 3/3\n","25000/25000 [==============================] - 286s 11ms/step - loss: 0.1970 - acc: 0.9252\n","Accuracy: 88.03%\n"],"name":"stdout"}]},{"metadata":{"id":"8wEnDqTrcAUQ","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}